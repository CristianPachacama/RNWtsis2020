\subsection{Algoritmos de Agrupamiento}

Una vez que se determina la medida de disimilitud, se obtiene una matriz de disimilitud inicial (que contiene la disimilitud entre parejas de series), y luego se usa un algoritmo de agrupamiento convencional para formar los clústers (grupos) con las series. De hecho, la mayoría de los enfoques de agrupamiento de series de tiempo desarrollados por autores como \citeauthor{liao2005clustering} son variaciones de procedimientos generales como por ejemplo: K-Means, K-Medoids, PAM, CLARA \cite{kaufman1986clustering} o de Clúster jerárquico, que utilizan una gama de disimilitudes específicamente diseñadas para tratar con series de tiempo y algunas de sus características. A continuación detallamos dos de los algoritmos que usaremos en nuestro análisis.

\subsubsection{Particionamiento alrededor de Medoides (PAM)}



%Por ejemplo si un objeto $O_i$ es el medoide de un grupo, entonces diremos que el objeto $O_j$ pertenece a dicho grupo si $d(O_j , O_i) = \underset{O_m}{\min} \{ d(O_j,O_m) \}$

%\textbf{Observación. } La calidad de agrupamiento del método se mide como la distancia promedio entre los objetos y sus respectivos medoides.

%El algoritmo PAM halla los $k$ medoids a partir de un conjunto arbitrario de objetos para luego intercambiarlos sucesivamente de tal manera de que en cada paso se mejore la calidad de agrupamiento.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


El algoritmo PAM propuesto por \citeauthor{rousseeuw1990finding}, tiene por objetivo hallar $k$ grupos (clústers) a partir de $n$ de objetos , esto mediante la identificación de objetos representativos que están lo más centralmente localizados dentro de cada grupo, estos objetos se conocen como "medoides". El algoritmo consiste de dos fases, en una primera fase, 
%conocida como BUILD (Construcción), 
se obtiene un agrupamiento inicial mediante la selección sucesiva de objetos representativos hasta que se hayan encontrado $k$ objetos. El primer objeto es aquel para el cual la suma de las diferencias con todos los demás objetos es lo más pequeña posible. Este objeto es el más centralmente ubicado en el conjunto de objetos. Posteriormente, en cada paso se selecciona otro objeto, este objeto es aquel que disminuye la función objetivo tanto como sea posible. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Por ejemplo, para medir el efecto de intercambiar un objeto $O_i_1$ por $O_i_2$ el algoritmo PAM calcula el costo $C_j (i_1,i_2)$ (para todo objeto $O_j$ no selecionado. $C_j (i_1,i_2)$  se calcula según cada uno de los siguientes casos:


\textbf{Algoritmo}

\begin{enumerate}
\item Seleccionar $k$ objetos arbitrariamente
\item Calcular $T(i,h)$ para todos los pares de objetos, tales que $O_i$ está seleccionado y $O_h$ no. Para el cálculo de $T(i,h)$ se considera lo siguiente:


\begin{enumerate}

\item  Supongamos que $O_j$ pertenece al grupo representado por el medoide $O_i$. Luego supongamos que $O_j$ es más parecido a $O_{k}$ que $O_h$. Así, si reemplazamos $O_i$ por $O_h$ como medoide del grupo, entonces $O_j$ pertenecería al grupo representado por $O_{k}$. Por lo tanto el costo de intercambio de medoides respecto de $O_j$ es :

$$C_{j}(i,h)= d(O_j,O_{k})-d(O_j,O_i)$$

Notemos que $C_{j}(i,h)\geq 0$ 

\item Supongamos que $O_j$ pertenece al grupo representado por el medoide $O_i$. Pero esta vez $O_j$ es menos parecido a $O_{k}$ que $O_h$. Así, el costo de  reemplazar $O_i$ por $O_h$ viene dado por:

$$C_{j}(i,h)= d(O_j,O_{h})-d(O_j,O_i)$$

En este caso $C_{j}(i,h)$ puede ser positivo o negativo.

\item Supongamos que $O_j$ pertenece a un grupo distinto al representado por el medoide $O_i$ . Sea $O_{k}$ el medoide de ese grupo. Luego supongamos que $O_j$ es más similar a $O_{k}$ que a $O_h$, entonces:

$$C_{j}(i,h)= 0$

\item Supongamos que $O_j$ pertenece al grupo representado por el medoide $O_i$. Entonces reeplazar $O_i$ con $O_h$ provocaría que $O_j$ pase del grupo representado por $O_h$ al grupo representado por $O_k$. Así, el costo viene dado por:

$$C_{j}(i,h) = d(O_j,O_{h})-d(O_j,O_k)$$

Notemos que $C_{j}(i,h)<0$

\item Finalmente el costo total de reemplazar $O_i$ por $O_h$ está dado por:

$$T(i,h)= \sum_{i} C_{j}(i,h)$$

\end{enumerate} 





\item Seleccionar el par $O_i, O_h$ que minimice $T(i,h)$. Si el mínimo $T(i,h)$ es negativo, reemplazar $O_i$ con $O_h$ y vuelva al paso 2. 
\item Caso contrario, para cada objeto no seleccionado, hallar el medoide más parecido.  
\end{enumerate}

\paragraph{Observación.} Resultados experimentales obtenidos en \cite{rousseeuw1990finding} muestran que PAM funciona adecuadamente con conjuntos de datos pequeños (100 objetos), pero no es eficiente para grandes conjuntos de datos, lo que se puede evidenciar al analizar la complejidad del algoritmo PAM, donde se puede observar que cada iteración del algoritmo tiene un orden de complejidad de $O(k(n-k)^2)$.











\subsubsection{CLARA}
CLARA (Clustering Large Aplications) es un método desarrollado por Kaufman y Rousseuw con la finalidad de agrupar un gran número de objetos. El algoritmo CLARA consiste básicamente  en aplicar PAM sobre una muestra aleatoria de objetos, en lugar de aplicarlo directamente a todos los objetos. Este algoritmo está motivado partiendo del supuesto  de que los medoides de una muestra de objetos tomada aleatoriamente, aproximaría a los medoides de todos los objetos. Para mejorar esta aproximación CLARA toma $L$ muestras  y devuelve la mejor agrupación. En este caso, la calidad de agrupamiento se mide como la distancia promedio entre todos los objetos y sus medoides (no solo los de la muestra).

\textbf{Algoritmo}

\begin{enumerate}
\item Realizar $L$ veces lo siguiente:
\item Tomar una muestra aleatoria de $m$ de los $n$ objetos , y ejecutar el algoritmo PAM para hallar los $k$ medoides de esta muestra.
\item Para cada objeto $O_j$ en la data completa (no solo en la muestra), determinar cual de los $k$ medoides es el más similar a $O_j$.
\item Calcular la disimilitud promedio del grupo obtenido en el paso anterior. Si este valor es menor al mínimo anterior, actualizamos el valor mínimo y guardamos los $k$ medoides del paso 2 como los mejores medoides obtenidos hasta el momento.

\end{enumerate}

Corridas experimentales realizadas en \cite{rousseeuw1990finding} muestran que tomar $L=5$ muestras de tamaño $m=40 + 2k$ da buenos resultados. 

\paragraph{Observación. } Se puede corroborar que el orden de complejidad del algoritmo CLARA es $O(k(40+k)^2+k(n-k))$, esto explica porque CLARA es más eficiente que PAM para valores grandes de $n$.

\paragraph{Nota.}

La implementación en el lenguaje de programación R, tanto del algoritmo PAM como CLARA, se encuentran en el paquete \textit{cluster} desarrollado por \citeauthor{clust2019r}. Mientras que los gráficos asociados al análisis clúster con estos dos algoritmos se encuentran en el paquete \textit{factoextra} desarrollado por \citeauthor{factoext2017r}

%\subsection{CLARANS}
%\cite{ng2002clarans}

\subsection{Validación}

Una etapa adicional dentro del análisis clúster consiste en determinar la cantidad de clústers que es más apropiada para los datos. Idealmente, los clústers resultantes no solo deberían tener buenas propiedades estadísticas (compactas, bien separadas, conectadas y estables), sino también resultados relevantes. Se han propuesto una variedad de medidas y métodos para validar los resultados de un análisis clúster y determinar tanto el número de clústers, así como identificar qué algoritmo de agrupamiento ofrece el mejor rendimiento, algunas de estas ellas pueden encontrarse en \citeauthor{fraley1998many} \citeyear{fraley1998many}, \citeauthor{duda2001pattern} \citeyear{duda2001pattern}, \citeauthor{salvador2004determining} \citeayear{salvador2004determining}, y \citeauthor{kerr2001bootstrapping} \citeyear{kerr2001bootstrapping}. 


A continuación, mostramos un estadístico llamado GAP, desarrollado por \citeauthor{tibshirani2001estimating} \citeyear{tibshirani2001estimating}, mismo que considera la dispersión en y entre clústers, que por su versatilidad y fácil estimación puede usarse en la elección del número de clústers para cualquier algoritmo de agrupamiento.

\subsubsection{Estadístico GAP}

Supongamos que a partir de $n$ objetos hemos creado $k$ clústers $C_1,\dots,C_k$, cada clúster $C_r$ posee $n_r$ objetos, y definamos:

\[
D_r = \sum_{O_i,O_j\in C_r} \delta_{ij} 
\]

La suma de las distancias dos a dos de todos los objetos del clúster $r$. Luego, consideremos 

\[
W_k = \sum_{r=1}^k \frac{1}{2n_r}D_r
\]

Así $W_k$ es la suma de cuadrados dentro del clúster, alrededor de los centro del Clúster. Luego, se estandariza  $log(W_k)$ comparandolo con su esperanza bajo una distribución nula de referencia de los datos.

\[
\text{Gap}_n(k) = E_n^*(log(W_k))-log(W_k)
\]

Donde $E_n^*$ denota la esperanza bajo un tamaño de muestra $n$ de la distribución de referencia. El valor estimado $\hat k$ será aquel valor que maximice $\text{Gap}_n(k)$.

